# -*- coding: utf-8 -*-
"""model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rJrCz4QFc8Fac4lHRaio1ifvTLNscfwA
"""

from google.colab import drive
drive.mount('/content/drive')

import os


data_path = '/content/drive/MyDrive/dataset/9) Car-Hacking Dataset'
os.listdir(data_path)

import pandas as pd
import re
import numpy as np
import os

def parse_line(line):

    regex = r"Timestamp:\s*(\d+\.\d+)\s+ID:\s*(\w+)\s+000\s+DLC:\s*(\d+)\s+([\da-fA-F\s]+)"

    match = re.match(regex, line.strip())
    if match:

        timestamp = float(match.group(1))
        can_id = match.group(2)
        dlc = int(match.group(3))
        data = match.group(4).split()



        can_id_int = int(can_id, 16)

        data_int = [int(byte, 16) for byte in data]




        return {
            'Timestamp': timestamp,
            'CAN_ID': can_id_int,
            'DLC': dlc,
            'DATA': data_int
        }
    else:
        return None


file_path = '/content/drive/MyDrive/dataset/9) Car-Hacking Dataset/normal_run_data/normal_run_data.txt'
data = []

with open(file_path, 'r') as f:
    for line in f:
        parsed_data = parse_line(line)
        if parsed_data:
            data.append(parsed_data)

df = pd.DataFrame(data)

max_dlc = df['DLC'].max()


for i in range(8):
    df[f'DATA{i}'] = df['DATA'].apply(lambda x: x[i] if i < len(x) else 0)


df = df.drop(columns=['DATA'])

print(df.head())

df['Flag'] = 'R'

df.head()

column_names = [
    'Timestamp', 'CAN_ID', 'DLC',
    'DATA0', 'DATA1', 'DATA2', 'DATA3', 'DATA4', 'DATA5', 'DATA6', 'DATA7',
    'Flag'
]

dos_path = '/content/drive/MyDrive/dataset/9) Car-Hacking Dataset/dos_attack.csv'
fuzzy_path = '/content/drive/MyDrive/dataset/9) Car-Hacking Dataset/fuzzy_attack.csv'
gear_spoofing_path = '/content/drive/MyDrive/dataset/9) Car-Hacking Dataset/gear_spoofing.csv'
rpm_spoofing_path = '/content/drive/MyDrive/dataset/9) Car-Hacking Dataset/rpm_spoofing.csv'

df2 = pd.read_csv(dos_path, header=None, names=column_names, skipinitialspace=True)

df3=pd.read_csv(fuzzy_path, header=None, names=column_names, skipinitialspace=True)

df4=pd.read_csv(gear_spoofing_path, header=None, names=column_names, skipinitialspace=True)



df5=pd.read_csv(rpm_spoofing_path, header=None, names=column_names, skipinitialspace=True)

df5

df2.head()

df2.info()

def preprocess_flag(df):
    """Map Flag column to numeric: R -> 1 (normal), T -> 0 (abnormal)."""
    y = df['Flag'].copy()
    if pd.api.types.is_numeric_dtype(y):
        return y.fillna(0).astype(np.int32)
    y = y.replace({'R': 1, 'T': 0})
    return y.fillna(0).astype(np.int32)


def hex_to_onehot(hex_char):
    """
    Converts a single hexadecimal character into a one-hot vector.
    """
    hex_char = str(hex_char).lower()
    hex_map = {char: i for i, char in enumerate("0123456789abcdef")}
    if hex_char not in hex_map:
        raise ValueError(f"Invalid hex character: {hex_char!r}")
    onehot = np.zeros(16, dtype=np.int32)
    onehot[hex_map[hex_char]] = 1
    return onehot

def can_id_to_image(can_id):
    """
    Converts a hexadecimal CAN ID into a CAN data image (16x3 matrix).
    Expects can_id as a string of 1â€“3 hex chars; pads to 3 for consistent shape.
    """
    can_id = str(can_id).strip().lower().replace("0x", "")
    can_id = (can_id or "0").zfill(3)[-3:]
    onehot_vectors = [hex_to_onehot(c) for c in can_id]
    can_image = np.stack(onehot_vectors, axis=1)
    return can_image


def preprocess_can_data(df):
    """
    Converts a DataFrame of CAN IDs into CAN images (16x3x1).
    CAN IDs are normalized to exactly 3 hex chars for consistent image shape.
    """
    def normalize_can_id(x):
        if isinstance(x, (int, np.integer)):
            h = f"{int(x) & 0xFFF:x}".zfill(3)[-3:]
        else:
            h = re.sub(r"^0x", "", str(x).strip().lower())
            h = (h or "0").zfill(3)[-3:]
        return h

    df = df.copy()
    df["CAN_ID"] = df["CAN_ID"].apply(normalize_can_id)
    can_images = np.array([can_id_to_image(can_id) for can_id in df["CAN_ID"]])
    can_images = can_images[..., np.newaxis]
    return can_images

normal_images = preprocess_can_data(df)

normal_images.shape

normal_images[0]

train_images = normal_images

dos_images= preprocess_can_data(df2)

dos_images.shape

fuzzy_images= preprocess_can_data(df3)

fuzzy_images.shape

import tensorflow as tf

def batch_resize_to_fixed_shape_np(batch_images, target_shape=(16, 3, 1)):

    target_height, target_width, target_channels = target_shape

    if isinstance(batch_images, np.ndarray):
        batch_images = tf.convert_to_tensor(batch_images)

    resized_batch = tf.image.resize(batch_images, (target_height, target_width))

    if resized_batch.shape[-1] != target_channels:
        resized_batch = tf.expand_dims(resized_batch, axis=-1)

    return resized_batch.numpy()

half_index = fuzzy_images.shape[0] // 2
fuzzy_images = fuzzy_images[:half_index]

fuzzy_images=batch_resize_to_fixed_shape_np(fuzzy_images)

half_index2 = dos_images.shape[0] // 2
dos_images = dos_images[:half_index2]

dos_images=batch_resize_to_fixed_shape_np(dos_images)

fuzzy_images.shape

dos_images.shape

#gear_spoofing_images= preprocess_can_data(df4)

#rpm_spoofing_images= preprocess_can_data(df5)

from sklearn.preprocessing import OneHotEncoder
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, Reshape, BatchNormalization, ReLU, Flatten, LeakyReLU, Input
from tensorflow.keras.optimizers import Adam

# Generator Model
def create_generator(latent_dim, output_shape):
    model = Sequential([
        Dense(256, activation='relu', input_dim=latent_dim),
        BatchNormalization(),
        Dense(512, activation='relu'),
        BatchNormalization(),
        Dense(np.prod(output_shape), activation='tanh'),
        Reshape(output_shape)
    ])
    return model

latent_dim = 100
output_shape = train_images.shape[1:]
generator = create_generator(latent_dim, output_shape)
generator.summary()

from tensorflow.keras.initializers import HeNormal

def create_discriminator(input_shape):
    model = Sequential([
        Flatten(input_shape=input_shape),
        Dense(128, kernel_initializer=HeNormal()),
        LeakyReLU(0.2),
        Dense(64, kernel_initializer=HeNormal()),
        LeakyReLU(0.2),
        Dense(1, activation='sigmoid', kernel_initializer=HeNormal())
    ])
    return model
input_shape = train_images.shape[1:]
discriminator_known = create_discriminator(input_shape)
discriminator_unknown = create_discriminator(input_shape)
discriminator_known.summary()
discriminator_unknown.summary()

# Reward function for generator
def calculate_reward(discriminator, fake_images):
    """
    Calculate rewards based on the discriminator's predictions for fake images.
    """
    predictions = discriminator.predict(fake_images)
    return np.log(predictions + 1e-8)

# RL-based generator update
def train_generator_with_rl(generator, discriminator, latent_dim, batch_size, rl_lr=0.0001):
    noise = np.random.normal(0, 1, (batch_size, latent_dim)).astype(np.float32)

    with tf.GradientTape() as tape:
        fake_images = generator(noise, training=True)
        rewards = calculate_reward(discriminator, fake_images)
        rewards = tf.reshape(rewards, [-1, 1, 1, 1])
        # Log of (fake_images + eps) for policy gradient; clip to avoid log(0)
        log_probs = tf.math.log(tf.clip_by_value(fake_images, 1e-7, 1.0) + 1e-8)
        policy_loss = -tf.reduce_mean(log_probs * rewards)

    gradients = tape.gradient(policy_loss, generator.trainable_variables)
    if gradients is not None:
        generator.optimizer.apply_gradients(zip(gradients, generator.trainable_variables))

generator = create_generator(latent_dim, output_shape)
discriminator_known = create_discriminator(output_shape)
discriminator_unknown = create_discriminator(output_shape)

discriminator_known_optimizer = Adam(learning_rate=0.0002, clipvalue=1.0)
discriminator_unknown_optimizer = Adam(learning_rate=0.0002, clipvalue=1.0)
def stable_binary_crossentropy(y_true, y_pred):
    y_pred = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)
    return tf.keras.losses.binary_crossentropy(y_true, y_pred)


discriminator_known.compile(optimizer=discriminator_known_optimizer, loss=stable_binary_crossentropy, metrics=['accuracy'])
discriminator_unknown.compile(optimizer=discriminator_unknown_optimizer, loss=stable_binary_crossentropy, metrics=['accuracy'])

generator.compile(optimizer=Adam(0.0002), loss='binary_crossentropy')

def check_nan_in_images(fuzzy_images):
    if np.any(np.isnan(fuzzy_images)):
        print("NaN values found in fuzzy_images!")
    else:
        print("No NaN values in fuzzy_images.")


def check_nan_in_labels(abnormal_flags):
    if np.any(np.isnan(abnormal_flags)):
        print("NaN values found in abnormal_flags!")
    else:
        print("No NaN values in abnormal_flags.")


def check_flag_dimensions(abnormal_flags, half_batch):
    if abnormal_flags.shape != (half_batch, 1):
        print(f"Warning: abnormal_flags shape mismatch. Expected: ({half_batch}, 1), Found: {abnormal_flags.shape}")
    else:
        print("abnormal_flags shape is correct.")

X_train = normal_images
latent_dim = 100
output_shape = X_train.shape[1:]





epochs = 500
batch_size = 64
half_batch = batch_size // 2

for epoch in range(epochs):

    idx = np.random.randint(0, X_train.shape[0], half_batch)
    real_images = X_train[idx]
    real_labels = np.ones((half_batch, 1))


    idx_fuzzy = np.random.randint(0, fuzzy_images.shape[0], half_batch)
    abnormal_images = fuzzy_images[idx_fuzzy]


    abnormal_flags = preprocess_flag(df3.iloc[idx_fuzzy])
    abnormal_labels = abnormal_flags.values.reshape(-1, 1)
    combined_images_known = np.concatenate([real_images, abnormal_images], axis=0)
    combined_labels_known = np.concatenate([real_labels, abnormal_labels], axis=0)
    d_loss_known = discriminator_known.train_on_batch(combined_images_known, combined_labels_known)


    noise = np.random.normal(0, 1, (half_batch, latent_dim))
    fake_images = generator.predict(noise)
    fake_labels = np.zeros((half_batch, 1))
    combined_images_unknown = np.concatenate([real_images, fake_images], axis=0)
    combined_labels_unknown = np.concatenate([real_labels, fake_labels], axis=0)
    d_loss_unknown = discriminator_unknown.train_on_batch(combined_images_unknown, combined_labels_unknown)


    train_generator_with_rl(generator, discriminator_unknown, latent_dim, batch_size)



    print(f"1st D (Known):{d_loss_known[1]}")
    print(f"2nd D (Unknown):{d_loss_unknown[1]}")

threshold_known = 0.5
threshold_unknown = 0.5

def classify_can_data(can_images, discriminator_known, discriminator_unknown, threshold_known, threshold_unknown):

    can_images = np.array(can_images)
    if can_images.ndim == 3:
        can_images = np.expand_dims(can_images, axis=0)

    classifications = []

    outputs_known = discriminator_known.predict(can_images)
    outputs_unknown = discriminator_unknown.predict(can_images)

    for i in range(len(can_images)):
        output_known = outputs_known[i][0]
        if output_known < threshold_known:
            classifications.append("Abnormal")
        else:
            output_unknown = outputs_unknown[i][0]
            if output_unknown < threshold_unknown:
                classifications.append("Abnormal")
            else:
                classifications.append("Normal")

    return classifications


test_images = dos_images[:10]
results = classify_can_data(test_images, discriminator_known, discriminator_unknown, threshold_known, threshold_unknown)

for i, result in enumerate(results):
    print(f"CAN Image {i + 1}: {result}")

def calculate_accuracy(predictions, true_labels):


    prediction_numeric = np.array([1 if label == "Normal" else 0 for label in predictions])


    correct_predictions = np.sum(prediction_numeric == true_labels)


    accuracy = correct_predictions / len(true_labels) * 100

    return accuracy

half_dos_images = dos_images[:len(dos_images)//2]


half_true_labels = preprocess_flag(df2[:len(dos_images)//2]).values

half_predictions = classify_can_data(half_dos_images, discriminator_known, discriminator_unknown, threshold_known, threshold_unknown)

half_accuracy = calculate_accuracy(half_predictions, half_true_labels)

print(f"Accuracy on half of the DDoS test data: {half_accuracy}%")